<<<<<<< HEAD
# Implementing a Transformer
The aim of this project is to implement the transformer architecture. [Transformer Paper](https://arxiv.org/abs/1706.03762)

One of the main goals to do this is to understand the workings of multi headed attention.
Other goals are to build a good standalone project following industry standards, and using industry adopted technologies.

There are numerous implementations of the transformer architecture, and this by no means is aimed for education purposes or experiments.
=======
# Attention_Is_All_You_Need
Implementing the original Transformer network from scratch in PyTorch. Training it to be a MT system for English to Hindi using the IIT B dataset.
>>>>>>> origin/main
